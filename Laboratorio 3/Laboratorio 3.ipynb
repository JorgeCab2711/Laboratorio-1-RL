{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e84281a-0828-47e0-80d1-67bc19c1e05e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "- Jorge Caballeros Pérez - 20009\n",
    "- Mario de León - 19019\n",
    "- Pablo Escobar - 20936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d12fe7-2574-4714-b77b-c9a4e901c2b5",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "### ¿Qué es Programación Dinámica y cómo se relaciona con RL?\n",
    "Una técnica para resolver problemas complejos dividiéndolos en subproblemas más simples y guardando sus soluciones para evitar recalcularlas. Se usa para encontrar políticas óptimas en problemas de decisión secuencial, aprovechando el conocimiento del modelo del entorno (transiciones y recompensas)\n",
    "\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Póliza.\n",
    "El algoritmo alterna entre dos fases:\n",
    "\n",
    "- Evaluación de la póliza: calcula el valor de cada estado bajo una póliza dada\n",
    "- Mejora de la póliza: ajusta la póliza para elegir acciones que maximicen el valor calculado\n",
    "\n",
    "Se repite hasta que la póliza se vuelve óptima\n",
    "\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "El algoritmo actualiza directamente los valores de los estados:\n",
    "\n",
    "- Actualización de valor: calcula el valor máximo esperado para cada estado tomando en cuenta todas las acciones posibles\n",
    "- Extracción de la póliza: calcula la póliza óptima eligiendo la mejor acción basada en los valores actualizados\n",
    "\n",
    "Se repite hasta que los valores convergen\n",
    "\n",
    "### En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?\n",
    "Los premios se mantienen constantes para simplificar el análisis y la implementación de los algoritmos. Así hay una convergencia predecible y eficiente al buscar una poliza óptima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212037f-7e00-4abf-b488-267827f9e620",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "El objetivo principal de este ejercicio es que simule un MDP que represente un robot que navega por un laberinto de\n",
    "cuadrículas de 3x3 y evalúe una política determinada.\n",
    "Por ello considere, a un robot navega por un laberinto de cuadrícula de 3x3. El robot puede moverse en cuatro\n",
    "direcciones: arriba, abajo, izquierda y derecha. El objetivo es navegar desde la posición inicial hasta la posición de\n",
    "meta evitando obstáculos. El robot recibe una recompensa cuando alcanza la meta y una penalización si choca con\n",
    "un obstáculo. \n",
    "El laberinto es el siguiente:\n",
    "- [ S ] [_._] [ X ] \n",
    "- [_._] [ X ] [_._] \n",
    "- [_._] [_._] [ G ]\n",
    "\n",
    "Donde:\n",
    "- S = punto de inicio\n",
    "- G = punto de meta\n",
    "- X = son obstáculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38a5c53-d739-4dab-86b9-578d0e4ba18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b2422c-3e89-483a-b3f6-975647eccff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del robot\n",
    "class Robot:\n",
    "    def __init__(self, position: int, score: int = 0):\n",
    "        self.position = position\n",
    "        self.score = score\n",
    "    \n",
    "    def move(self, direction: str):\n",
    "        transitions = {\n",
    "            'arriba': -3,\n",
    "            'abajo': 3,\n",
    "            'izquierda': -1,\n",
    "            'derecha': 1\n",
    "        }\n",
    "        if direction in transitions:\n",
    "            self.position += transitions[direction]\n",
    "            self.position = max(0, min(8, self.position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fb5179-cf05-41b8-8e2a-4f388e349e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del tablero\n",
    "class Board:\n",
    "    def __init__(self, layout: int = 3):\n",
    "        self.layout = layout\n",
    "        self.states = [[' ' for _ in range(self.layout)] for _ in range(self.layout)]\n",
    "        \n",
    "    def setObstacles(self, xPos, yPos):\n",
    "        self.states[xPos][yPos] = 'X'\n",
    "    \n",
    "    def setPrize(self, xPos, yPos):\n",
    "        self.states[xPos][yPos] = 'G'\n",
    "    \n",
    "    def setPlayer(self, xPos, yPos):\n",
    "        self.states[xPos][yPos] = 'S'\n",
    "    \n",
    "    def printBoard(self):\n",
    "        for row in self.states:\n",
    "            for element in row:\n",
    "                print(f'[ {element} ]', end=\" \")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36b8590-df5d-4b6f-b235-0c5e174ea58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar tablero\n",
    "board = Board(3)\n",
    "board.setPlayer(0, 0)    # S en (0, 0)\n",
    "board.setObstacles(0, 2) # X en (0, 2)\n",
    "board.setObstacles(1, 1) # X en (1, 1)\n",
    "board.setPrize(2, 2)     # G en (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acb4ed5-1b39-494f-80a0-5022b83f95e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ S ] [   ] [ X ] \n",
      "[   ] [ X ] [   ] \n",
      "[   ] [   ] [ G ] \n"
     ]
    }
   ],
   "source": [
    "# Imprimir el tablero\n",
    "board.printBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a350bdb-767c-4998-95e5-1da4d08e6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del MDP\n",
    "states = list(range(9))\n",
    "actions = ['arriba', 'abajo', 'izquierda', 'derecha']\n",
    "\n",
    "# Transiciones\n",
    "def getTransitions(state):\n",
    "    transitions = {\n",
    "        'arriba': state - 3 if state >= 3 else state,\n",
    "        'abajo': state + 3 if state < 6 else state,\n",
    "        'izquierda': state - 1 if state % 3 != 0 else state,\n",
    "        'derecha': state + 1 if state % 3 != 2 else state\n",
    "    }\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea376f47-7df6-4da1-8976-41e1251bb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = {state: getTransitions(state) for state in states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bdce73-4009-46e6-91fd-d060c314ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompensas\n",
    "rewards = {state: {a: -1 for a in actions} for state in states}\n",
    "obstacles = [2, 4]\n",
    "goalState = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62e9683-a3f4-4b44-9d59-eb386202c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in obstacles:\n",
    "    rewards[state] = {a: -100 for a in actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01dac87-44a0-414f-8072-4412ce047fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[goalState] = {a: 10 for a in actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b28bf27-d245-4f08-9775-b99eff35b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in obstacles:\n",
    "    for action in actions:\n",
    "        transitions[state][action] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16328856-5b1e-4215-aa31-d13736351813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de la función de valor\n",
    "v = {state: 0 for state in states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c966e8-e47b-462c-9473-0137599215ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del algoritmo\n",
    "gamma = 0.9\n",
    "threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf23af2f-f2ed-43f7-8d05-3e3dd548e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de iteración de valor\n",
    "def valueIteration(states, actions, transitions, rewards, v, gamma, threshold):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v_old = v[s]\n",
    "            v[s] = max([rewards[s][a] + gamma * v[transitions[s][a]] for a in actions])\n",
    "            delta = max(delta, abs(v_old - v[s]))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9926aac4-0490-46ca-8c57-d66b25e74aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la política óptima\n",
    "def extractPolicy(states, actions, transitions, rewards, v, gamma):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        actionValues = {}\n",
    "        for a in actions:\n",
    "            actionValues[a] = rewards[s][a] + gamma * v[transitions[s][a]]\n",
    "        policy[s] = max(actionValues, key=actionValues.get)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25773bcc-df2d-4f59-9a29-8657aa3eb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de iteración de políticas\n",
    "def policyIteration(states, actions, transitions, rewards, gamma, threshold):\n",
    "    # Inicializar una política aleatoria\n",
    "    policy = {s: random.choice(actions) for s in states}\n",
    "    \n",
    "    def policyEvaluation(policy, states, transitions, rewards, v, gamma, threshold):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                v_old = v[s]\n",
    "                a = policy[s]\n",
    "                v[s] = rewards[s][a] + gamma * v[transitions[s][a]]\n",
    "                delta = max(delta, abs(v_old - v[s]))\n",
    "            if delta < threshold:\n",
    "                break\n",
    "        return v\n",
    "    \n",
    "    while True:\n",
    "        v = {state: 0 for state in states}\n",
    "        v = policyEvaluation(policy, states, transitions, rewards, v, gamma, threshold)\n",
    "        policyStable = True\n",
    "        for s in states:\n",
    "            oldAction = policy[s]\n",
    "            actionValues = {}\n",
    "            for a in actions:\n",
    "                actionValues[a] = rewards[s][a] + gamma * v[transitions[s][a]]\n",
    "            bestAction = max(actionValues, key=actionValues.get)\n",
    "            policy[s] = bestAction\n",
    "            if oldAction != bestAction:\n",
    "                policyStable = False\n",
    "        if policyStable:\n",
    "            break\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03c5087-9445-4f41-a6ba-d4c4cdfa415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar iteración de valor\n",
    "v = valueIteration(states, actions, transitions, rewards, v, gamma, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af134cf6-0bbf-4c02-81a9-331f871f5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer política óptima de la iteración de valor\n",
    "policyValueIteration = extractPolicy(states, actions, transitions, rewards, v, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e02b5a-8a57-4e00-a77b-9edbb29f42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar iteración de políticas\n",
    "policyPolicyIteration, vPolicyIteration = policyIteration(states, actions, transitions, rewards, gamma, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e889ef2e-2c96-4733-9fcd-bf8a23d124e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de Valor (Iteración de Valor):\n",
      "Estado 0: 62.170166475158226\n",
      "Estado 1: 54.953149827642406\n",
      "Estado 2: -999.9916647515823\n",
      "Estado 3: 70.1891664751582\n",
      "Estado 4: -999.9916647515823\n",
      "Estado 5: 88.99916647515822\n",
      "Estado 6: 79.09916647515821\n",
      "Estado 7: 88.99916647515822\n",
      "Estado 8: 99.99916647515822\n"
     ]
    }
   ],
   "source": [
    "print(\"Función de Valor (Iteración de Valor):\")\n",
    "for state in v:\n",
    "    print(f\"Estado {state}: {v[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684bed11-35ce-49a5-a7b7-7cae3c5eedfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Política Óptima (Iteración de Valor):\n",
      "Estado 0: abajo\n",
      "Estado 1: izquierda\n",
      "Estado 2: arriba\n",
      "Estado 3: abajo\n",
      "Estado 4: arriba\n",
      "Estado 5: abajo\n",
      "Estado 6: derecha\n",
      "Estado 7: derecha\n",
      "Estado 8: abajo\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPolítica Óptima (Iteración de Valor):\")\n",
    "for state in policyValueIteration:\n",
    "    print(f\"Estado {state}: {policyValueIteration[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5586d2-1363-45fb-b597-97a5e1b6ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Función de Valor (Iteración de Políticas):\n",
      "Estado 0: 62.170166475158226\n",
      "Estado 1: 54.953149827642406\n",
      "Estado 2: -999.9916647515823\n",
      "Estado 3: 70.1891664751582\n",
      "Estado 4: -999.9916647515823\n",
      "Estado 5: 88.99916647515822\n",
      "Estado 6: 79.09916647515821\n",
      "Estado 7: 88.99916647515822\n",
      "Estado 8: 99.99916647515822\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFunción de Valor (Iteración de Políticas):\")\n",
    "for state in vPolicyIteration:\n",
    "    print(f\"Estado {state}: {vPolicyIteration[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb6ea6b2-e360-4039-90de-bc33e262f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Política Óptima (Iteración de Políticas):\n",
      "Estado 0: abajo\n",
      "Estado 1: izquierda\n",
      "Estado 2: arriba\n",
      "Estado 3: abajo\n",
      "Estado 4: arriba\n",
      "Estado 5: abajo\n",
      "Estado 6: derecha\n",
      "Estado 7: derecha\n",
      "Estado 8: abajo\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPolítica Óptima (Iteración de Políticas):\")\n",
    "for state in policyPolicyIteration:\n",
    "    print(f\"Estado {state}: {policyPolicyIteration[state]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
